{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in UL2 tokenizer to see what's going on\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "new_tokens = [f'<new_id_{i}>' for i in range(200)]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "tokenizer.add_tokens(['[S2S]', '[NLU]', '[NLG]'])\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset red_pajama-data-1_t-sample (/Users/dylanskinner/Desktop/CS 674 Projects/MinGPT_UL2/datasets/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92e94a4532b42caa03490d0c7a033cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", 'plain_text', cache_dir='datasets')\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Custom dataset class for the Red Pajama dataset\n",
    "class RedPajamaDataset(Dataset):\n",
    "    def __init__(self, data, max_length=1024, ul2_switch=False):\n",
    "        self.data = data\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.add_tokens([f'new_id_{i}' for i in range(200)])\n",
    "        self.tokenizer.add_tokens(['[S2S]', '[NLU]', '[NLG]'])\n",
    "        self.tokenizer.pad_token_id = 50256\n",
    "        self.max_length = max_length - 1\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.token_dict = {'s': ['[S2S]', self._s_denoising], 'r': ['[NLU]', self._r_denoising], 'x': ['[NLG]', self._x_denoising]}\n",
    "        self.ul2_switch = ul2_switch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "\n",
    "        if self.ul2_switch:\n",
    "            # Get the token to prepend and function to use.\n",
    "            begin_id, func = self.token_dict[np.random.choice(['s', 'r', 'x'], size=1, p=[0.5, 0.25, 0.25])[0]]\n",
    "\n",
    "            # Prepend token to string and tokenize\n",
    "            text = begin_id + ' ' + text\n",
    "            ids = self.tokenizer.encode(text, truncation=True, max_length=self.max_length, return_tensors='pt', add_special_tokens=True, padding=True)\n",
    "\n",
    "            # Return the tokens\n",
    "            return func(ids, self.tokenizer)\n",
    "        \n",
    "        elif not self.ul2_switch:\n",
    "            # Tokenize the text\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_length, truncation=True, return_tensors='pt', padding=True)\n",
    "            \n",
    "            # Split the tokens into chunks of max_length\n",
    "            # Shift the tokens to get targets (excluding the [CLS] token)\n",
    "            target_tokens = tokens[:, 1:].clone()  # Exclude the [CLS] token\n",
    "            tokens = tokens[:, :-1]  # Exclude the last token to match the shifted targets\n",
    "            return tokens, target_tokens\n",
    "\n",
    "    # Helper functions! These will implement the UL2 tokenization.\n",
    "    def _r_denoising(self, ids, tokenizer, corruption_pct=0.15, span_length=np.arange(2, 6)):\n",
    "        # Calculate the chance of corruption based on the corruption percentage,\n",
    "        # mean span length, and the maximum span length\n",
    "        chance = (corruption_pct / np.mean(span_length)) * (1 + np.max(span_length) / ids.shape[1])\n",
    "        \n",
    "        # Variable to store the old tokens (before corruption)\n",
    "        old_toks = None\n",
    "\n",
    "        # Variables for tracking the number of steps to skip and tokens used\n",
    "        steps_to_skip = 0\n",
    "        tokens_used = 0\n",
    "\n",
    "        ids_shape = ids.shape[1]\n",
    "        print(ids_shape)\n",
    "\n",
    "        # Iterate through the tokens in the input sequence\n",
    "        for i in range(1, ids_shape):\n",
    "            # Skip steps if needed (due to recent corruption)\n",
    "            if steps_to_skip > 0:\n",
    "                steps_to_skip -= 1\n",
    "                continue\n",
    "\n",
    "            # Randomly decide whether to corrupt the current token\n",
    "            rnd = np.random.random()\n",
    "            if rnd < chance:\n",
    "                # Get the token used for masking (corruption)\n",
    "                mask_token = tokenizer.convert_tokens_to_ids(new_tokens[tokens_used])\n",
    "                tokens_used += 1\n",
    "                \n",
    "                # Randomly choose a span length for corruption\n",
    "                span = np.random.choice(span_length)\n",
    "\n",
    "                # Update old_toks and ids with the corrupted span\n",
    "                if old_toks is None:\n",
    "                    old_toks = torch.tensor([[mask_token]])  # Initialize old_toks with the first mask_token\n",
    "                    old_toks = torch.cat((old_toks, ids[:, i:i + span]), dim=1)  # Add the corrupted span to old_toks\n",
    "                    ids = torch.cat((ids[:, :i], torch.tensor([[mask_token]]), ids[:, i + span:]), dim=1)  # Mask the corrupted span in ids\n",
    "\n",
    "                    steps_to_skip = span\n",
    "                else:\n",
    "                    old_toks = torch.cat((old_toks, torch.tensor([[mask_token]]), ids[:, i:i + span]), dim=1)  # Add the corrupted span to old_toks\n",
    "                    ids = torch.cat((ids[:, :i], torch.tensor([[mask_token]]), ids[:, i + span:]), dim=1)  # Mask the corrupted span in ids\n",
    "\n",
    "                    # Update steps_to_skip to avoid overlapping corruption\n",
    "                    steps_to_skip = span\n",
    "\n",
    "        # Pad ids and old_toks to match the desired maximum length (R, X)\n",
    "        ids = torch.cat((ids, torch.tensor([[tokenizer.eos_token_id] * (self.max_length - ids.shape[1])])), dim=1)\n",
    "        old_toks = torch.cat((old_toks, torch.tensor([[tokenizer.eos_token_id] * (self.max_length - old_toks.shape[1])])), dim=1)\n",
    "\n",
    "        return ids, old_toks\n",
    "\n",
    "\n",
    "\n",
    "    def _s_denoising(self, ids, tokenizer):\n",
    "        # Get the length of our input\n",
    "        len_ids = ids.shape[1]\n",
    "\n",
    "        # Build Gaussian distribution of probabilities for each token\n",
    "        vals = np.linspace(-2, 2, len_ids)\n",
    "        p = norm.pdf(vals, loc=0, scale=1)\n",
    "\n",
    "        # Normalize the probabilities and get the index to remove\n",
    "        remove_index = np.random.choice(np.arange(len_ids // 2 - 15, len_ids // 2 + 15))\n",
    "\n",
    "        # Get the token we are using for this space\n",
    "        mask_token = tokenizer.convert_tokens_to_ids(new_tokens[0])\n",
    "\n",
    "        # Get the tokens we are removing\n",
    "        old_toks = torch.cat((torch.tensor([[mask_token]]), ids[:, remove_index:].clone()), dim=1)\n",
    "\n",
    "        # Mask the tokens\n",
    "        ids = ids[:, :remove_index + 1].clone()\n",
    "        ids[:, -1] = mask_token\n",
    "\n",
    "        # Pad ids and old_toks to match the desired maximum length (S)\n",
    "        ids = torch.cat((ids, torch.tensor([[tokenizer.eos_token_id] * (self.max_length - ids.shape[1])])), dim=1)\n",
    "        old_toks = torch.cat((old_toks, torch.tensor([[tokenizer.eos_token_id] * (self.max_length - old_toks.shape[1])])), dim=1)\n",
    "\n",
    "        return ids, old_toks\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def _x_denoising(self, ids, tokenizer, corruption_pct=0.50, span_length=np.arange(2,6)):\n",
    "        return self._r_denoising(ids, tokenizer, corruption_pct, span_length)\n",
    "    \n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "red_pajama_dataset_F = RedPajamaDataset(dataset)\n",
    "red_pajama_dataset_T = RedPajamaDataset(dataset, ul2_switch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.55M\n",
      "number of parameters: 2.55M\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "from mingpt.model import GPT\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'red_pajama'\n",
    "dir_path = f'./checkpoints/{checkpoint_dir}'\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    # If the directory doesn't exist, create it\n",
    "    os.makedirs(dir_path)\n",
    "    checkpoints = os.listdir(dir_path)\n",
    "else:\n",
    "    checkpoints = os.listdir(dir_path)\n",
    "\n",
    "checkpoints.sort()\n",
    "\n",
    "\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = red_pajama_dataset_T.vocab_size\n",
    "model_config.block_size = red_pajama_dataset_T.max_length\n",
    "# model_config.checkpoint = f'checkpoints/{checkpoint_dir}/' + checkpoints[-1] if checkpoints else None\n",
    "model_config.checkpoint = None\n",
    "model_config.use_ul2 = True\n",
    "model = GPT(model_config)\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = red_pajama_dataset_F.vocab_size\n",
    "model_config.block_size = red_pajama_dataset_F.max_length\n",
    "# model_config.checkpoint = f'checkpoints/{checkpoint_dir}/' + checkpoints[-1] if checkpoints else None\n",
    "model_config.checkpoint = None\n",
    "model_config.use_ul2 = False\n",
    "model2 = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n",
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "iters = 500\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = iters + model.iter_num if model_config.checkpoint else iters  # This is a change\n",
    "train_config.num_workers = 0\n",
    "train_config.checkpoint_iters = 100     # This is a change\n",
    "train_config.batch_size = 1\n",
    "train_config.checkpoint_name = f'{checkpoint_dir}/checkpoint_ul2'  # This is a change.\n",
    "trainer = Trainer(train_config, model, red_pajama_dataset_T)\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = iters + model.iter_num if model_config.checkpoint else iters  # This is a change\n",
    "train_config.num_workers = 0\n",
    "train_config.checkpoint_iters = 100     # This is a change\n",
    "train_config.batch_size = 1\n",
    "train_config.checkpoint_name = f'{checkpoint_dir}/checkpoint_break'  # This is a change.\n",
    "trainer2 = Trainer(train_config, model2, red_pajama_dataset_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/s5cpw8t92xbf0b9gw8j7ny2m0000gn/T/ipykernel_99306/2528016523.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Plot the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS 674 Projects/MinGPT_UL2/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# forward the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS 674 Projects/MinGPT_UL2/mingpt/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# forward the GPT model itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# token embeddings of shape (b, t, n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# position embeddings of shape (1, t, n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m         return F.embedding(\n\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()\n",
    "\n",
    "# Plot the loss\n",
    "losses = trainer.curr_loss\n",
    "x = 10\n",
    "new_losses = np.mean(np.array(losses).reshape(-1, x), axis=1)\n",
    "\n",
    "plt.plot(np.arange(len(new_losses)), new_losses, label='UL2')\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer2.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer2.iter_dt * 1000:.2f}ms; iter {trainer2.iter_num}: train loss {trainer2.loss.item():.5f}\")\n",
    "trainer2.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer2.run()\n",
    "\n",
    "# Plot the loss\n",
    "losses = trainer2.curr_loss\n",
    "x = 10\n",
    "new_losses = np.mean(np.array(losses).reshape(-1, x), axis=1)\n",
    "plt.plot(np.arange(len(new_losses)), new_losses, label='Regular')\n",
    "plt.title('Training of MinGPT on The Pile')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04296875"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corruption_pct = 0.15\n",
    "span_length = np.arange(2, 6)\n",
    "ids = torch.full((1, 1024), 1, dtype=torch.long)\n",
    "sample = np.random.random(ids.shape[1])\n",
    "np.sum(sample < (corruption_pct / np.mean(span_length)) * (1 + np.max(span_length) / ids.shape[1])) / ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NLU]\n",
      "Total non-tokenized torch.Size([1, 1024])\n",
      "Total tokenized torch.Size([1, 1024])\n",
      "[NLU] \\section{Introduction}\n",
      "\\ <new_id_0> ro}\n",
      "\n",
      "\\emph{Gender diversity}, or more often its lack thereof, among participants to\n",
      "software development activities has been thoroughly studied in recent years. In\n",
      "particular, the presence of, effects of, and countermeasures for \\emph{gender\n",
      "  bias} in Free/Open Source Software (FOSS) have received a lot of attention\n",
      "over the past decade~\\cite{david2008fossdevs, qiu2010kdewomen,\n",
      "  nafus2012patches, kuechler2012genderfoss, vasilescu2014gender,\n",
      "  oneil2016debiansurvey, robles2016womeninf <new_id_1>  terrell2017gender,\n",
      "  zacchiroli2021gender}.  \\emph{Geographic diversity} is on the other hand the\n",
      "kind of diversity that stems from participants in some global activity coming\n",
      "from different world regions and cultures.\n",
      "\n",
      "Geographic diversity in FOSS has received <new_id_2>  in scholarly\n",
      "works. In particular, while seminal survey <new_id_3>  and\n",
      "point-in-time medium-scale studies of the geographic origins of FOSS\n",
      "contributors exist~\\c <new_id_4> understanding, david2008fossdevs,\n",
      "  barahona2008 <new_id_5> akhteyev2010ossgeography, rob <new_id_6> dataset,\n",
      "  wachs2021ossgeography}, <new_id_7>  of the geographic\n",
      "origin of FOSS contributors are still lacking. Such a quantitative\n",
      "characterization would be useful to inform decisions related to global\n",
      "development teams~\\ <new_id_8> bsleb2007globalsweng} and hiring strategies in the\n",
      "information technology (IT) market, as well as contribute factual information\n",
      "to the debates on the economic impact and sociology of FOSS around the world.\n",
      "\n",
      "\n",
      "\\paragraph{Contributions}\n",
      "\n",
      "With this work we contribute <new_id_9>  conducting \\textbf{the first\n",
      "  longitudinal study of the geographic origin of contributors to public code\n",
      "  over 50 years.} Specifically, we provide a preliminary answer to the\n",
      "follow <new_id_10> \n",
      "\\begin{researchquestion}\n",
      "  From which world regions do authors of publicly available commits come from\n",
      " <new_id_11>  it changed over the past 50 years?\n",
      "  \\label{rq:geodiversity}\n",
      "\\end{researchquestion}\n",
      "We use as dataset the \\SWH/ archive~\\ <new_id_12> res2017} and analyze from it\n",
      "2.2 billion\\ <new_id_13>  archived from 160 million\\xspace projects and authored by\n",
      "43 <new_id_14> space authors during the 1971-- <new_id_15>  \n",
      "We geolocate developers to\n",
      "\\DATAWorldRegions/ world regions <new_id_16>  email country code top-level domains ( <new_id_17> LDs) and \n",
      "author (first/last) names compared with name distributions around the world, and UTC offsets \n",
      "mined from commit metadata.\n",
      "\n",
      "We find <new_id_18>  of North America in open source\n",
      "software, later joined by Europe. After that period, the geographic diversity  <new_id_19>  public code has been constantly increasing.\n",
      "We also identify relevant historical shifts\n",
      "related to the end of the UNIX wars and the increase of coding literacy in\n",
      "Central <new_id_20>  Asia, as well as of broader phenomena like colonialism and\n",
      "people movement across countries (immigration/emigration).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\paragraph{Data availability.}\n",
      "\n",
      "A replication package for this paper is available from Zenodo at\n",
      "\\url{https://doi.org/10.5281/zenodo.6390355}~ <new_id_21> ication-package}.\n",
      "\n",
      "\n",
      " \\section{Related Work}\n",
      "\\label{sec:related}\n",
      " <new_id_22>  recent works~\\cite{ghosh2005understanding, david2008fossdevs,\n",
      "  robles2014surveydat <new_id_23>  oneil2016debiansurvey} <new_id_24> ography of Free/Open Source Software (FOSS) using \\emph{developer surveys},\n",
      "which provide high-quality answers but are limited in size (2-5\\,K developers)\n",
      "and can be <new_id_25>.\n",
      "\n",
      "In 2008 Barahona et al.~\\cite{barahona2008geodiversity} conducted a seminal\n",
      "large-scale (for the time) study on FOSS \\emph{geography using mining software\n",
      "  repositories (MSR) techniques}. They analyzed the origin of 1\\,M contributors\n",
      "using the SourceForge user database and mailing list archives over the\n",
      "1999--2005 period, using as signals information similar to ours: email domains\n",
      "and UTC offsets. \n",
      "The studied period (7 <new_id_26> ~\\cite{barahona2008geodiversity} is <new_id_27> <new_id_28> <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/s5cpw8t92xbf0b9gw8j7ny2m0000gn/T/ipykernel_99306/3269594666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Total tokenized {old_toks.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_toks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3438\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m         )\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "new_tokens = [f'<new_id_{i}>' for i in range(200)]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "tokenizer.add_tokens(['[S2S]', '[NLU]', '[NLG]'])\n",
    "\n",
    "def r_denoising(ids, tokenizer, corruption_pct=0.15, span_length=np.arange(2,6)):\n",
    "    chance = (corruption_pct / np.mean(span_length)) * (1 + np.max(span_length) / ids.shape[1])\n",
    "    old_toks = None\n",
    "\n",
    "    steps_to_skip = 0\n",
    "    tokens_used = 0\n",
    "\n",
    "    for i in range(1, ids.shape[1]):\n",
    "        if steps_to_skip > 0:\n",
    "            steps_to_skip -= 1\n",
    "            continue\n",
    "\n",
    "        if np.random.random() < chance:\n",
    "            # Get the token we are using for this space\n",
    "            mask_token = tokenizer.convert_tokens_to_ids(new_tokens[tokens_used])\n",
    "            tokens_used += 1\n",
    "            span = np.random.choice(span_length)\n",
    "\n",
    "            if old_toks is None:\n",
    "                old_toks = torch.tensor([[mask_token]])\n",
    "                old_toks = torch.cat((old_toks, ids[:, i:i+span]), dim=1)\n",
    "                ids = torch.cat((ids[:, :i], torch.tensor([[mask_token]]), ids[:, i+span:]), dim=1)\n",
    "            else:\n",
    "                old_toks = torch.cat((old_toks, torch.tensor([[mask_token]]), ids[:, i:i+span]), dim=1)\n",
    "                ids = torch.cat((ids[:, :i], torch.tensor([[mask_token]]), ids[:, i+span:]), dim=1)\n",
    "\n",
    "            steps_to_skip = span\n",
    "        \n",
    "    # Pad ids and old_toks\n",
    "    ids = torch.concat((ids, torch.tensor([[tokenizer.eos_token_id] * (1024 - ids.shape[1])])), dim=1)\n",
    "    old_toks = torch.concat((old_toks, torch.tensor([[tokenizer.eos_token_id] * (1024 - old_toks.shape[1])])), dim=1)\n",
    "\n",
    "    return ids, old_toks\n",
    "\n",
    "def s_denoising(ids, tokenizer):\n",
    "    # Get the length of our input\n",
    "    len_ids = ids.shape[1]\n",
    "\n",
    "    # Build Gaussian distribution of probabilities for each token\n",
    "    vals = np.linspace(-2, 2, len_ids)\n",
    "    p = norm.pdf(vals, loc=0, scale=1)\n",
    "\n",
    "    # Normalize the probabilities and get the index: to remove.\n",
    "    remove_index = np.random.choice(np.arange(len_ids//2 - 15, len_ids//2 + 15))\n",
    "\n",
    "    # Get the token we are using for this space\n",
    "    mask_token = tokenizer.convert_tokens_to_ids(new_tokens[0])\n",
    "\n",
    "    # Get the tokens we are removing\n",
    "    old_toks = torch.concat((torch.tensor([[mask_token]]), ids[:, remove_index:].clone()), dim=1)\n",
    "\n",
    "    # Mask the tokens\n",
    "    ids = ids[:, :remove_index+1].clone()\n",
    "    ids[:, -1] = mask_token\n",
    "    ids = torch.concat((ids, torch.tensor([[tokenizer.eos_token_id] * (1024 - ids.shape[1])])), dim=1)\n",
    "    old_toks = torch.concat((old_toks, torch.tensor([[tokenizer.eos_token_id] * (1024 - old_toks.shape[1])])), dim=1)\n",
    "    return ids, old_toks\n",
    "\n",
    "def x_denoising(ids, tokenizer, corruption_pct=0.50, span_length=np.arange(2,6)):\n",
    "    return r_denoising(ids, tokenizer, corruption_pct, span_length)\n",
    "\n",
    "\n",
    "\n",
    "item = next(iter(dataset))['text']\n",
    "\n",
    "'''Testing R-Denoising'''\n",
    "# ids, old_toks = r_denoising(ids, tokenizer, corruption_pct=0.15, span_length=np.arange(2,6))\n",
    "# old_toks\n",
    "\n",
    "'''Testing S-Denoising'''\n",
    "# ids, old_tok = s_denoising(ids, tokenizer)\n",
    "# tokenizer.decode(ids)\n",
    "\n",
    "'''Testing X-Denoising'''\n",
    "# ids, old_toks = x_denoising(ids, tokenizer, corruption_pct=0.50, span_length=np.arange(2,6))\n",
    "# tokenizer.decode(ids)\n",
    "\n",
    "'''Testing all at once'''\n",
    "token_dict = {'s': ['[S2S]', s_denoising], 'r': ['[NLU]', r_denoising], 'x': ['[NLG]', x_denoising]}\n",
    "\n",
    "# Get the token to prepend and function to use.\n",
    "begin_id, func = token_dict[np.random.choice(['s', 'r', 'x'], size=1, p=[0.5, 0.25, 0.25])[0]]\n",
    "print(begin_id)\n",
    "\n",
    "# Prepend token to string and tokenize\n",
    "item = begin_id + ' ' + item\n",
    "ids = tokenizer(item, truncation=True, max_length=1024, return_tensors='pt')['input_ids']\n",
    "\n",
    "\n",
    "ids, old_toks = func(ids, tokenizer)\n",
    "\n",
    "print('Total non-tokenized', ids.shape)\n",
    "print(f'Total tokenized {old_toks.shape}')\n",
    "print(tokenizer.decode(ids[0]))\n",
    "tokenizer.decode(old_toks)\n",
    "\n",
    "\n",
    "# old_toks_dec = [tokenizer.decode(tok) for tok in old_toks]\n",
    "# old_toks_dec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
