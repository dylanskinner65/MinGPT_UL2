{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in UL2 tokenizer to see what's going on\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset red_pajama-data-1_t-sample (/Users/dylanskinner/Desktop/CS 674 Projects/MinGPT_UL2/datasets/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa36325491f14d3bad3c39023a93a00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", 'plain_text', cache_dir='datasets')\n",
    "dataset = dataset['train']\n",
    "\n",
    "# Custom dataset class for the Red Pajama dataset\n",
    "class RedPajamaDataset(Dataset):\n",
    "    def __init__(self, data, max_length=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.add_tokens([f'new_id_{i}' for i in range(200)])\n",
    "        self.tokenizer.add_tokens(['[S2S]', '[NLU]', '[NLG]'])\n",
    "        self.tokenizer.pad_token_id = 50256\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        # Tokenize the text\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_length, truncation=True, return_tensors='pt', padding=True)\n",
    "        # Split the tokens into chunks of max_length\n",
    "        # Shift the tokens to get targets (excluding the [CLS] token)\n",
    "        target_tokens = tokens[:, 1:].clone()  # Exclude the [CLS] token\n",
    "        tokens = tokens[:, :-1]  # Exclude the last token to match the shifted targets\n",
    "        return tokens, target_tokens\n",
    "    \n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "red_pajama_dataset = RedPajamaDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 'nice', 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "ff = ff[:3] + ['nice'] + ff[4:]\n",
    "\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-tokenized 953\n",
      "Total tokenized 139\n",
      "Total tokens 1092\n",
      "[NLU]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'new_id_0 \\\\section new_id_1 }\\n\\\\label new_id_2. In\\nparticular new_id_3 measures for \\\\em new_id_4 ~\\\\cite new_id_5 afus new_id_6,\\n  new_id_7  that stems from participants new_id_8  In particular, new_id_9 -scale studies of the new_id_10 2014surveydatas new_id_11 scale longitudinal studies new_id_12 } and new_id_13 Contributions new_id_14  world regions new_id_15 researchquestion} new_id_16 hipres new_id_17 \\nWe geol new_id_18 first/last) new_id_19  of North new_id_20.}\\n\\nA replication new_id_21 \\n\\\\ new_id_22 replication new_id_23  david2008foss new_id_24  Software (FOSS) new_id_25 In 2008 Barah new_id_26 geodiversity} conducted new_id_27  similar to ours new_id_28  (7 years) in new_id_29 ona2008geodiversity new_id_30 new_id_31 new_id_32 new_id_33'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = [f'new_id_{i}' for i in range(200)]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "tokenizer.add_tokens(['[S2S]', '[NLU]', '[NLG]'])\n",
    "\n",
    "def r_denoising(ids, tokenizer, corruption_pct=0.15, span_length=np.arange(2,6)):\n",
    "    chance = (corruption_pct / np.mean(span_length)) * (1 + np.max(span_length) / len(ids))\n",
    "    old_toks = []\n",
    "\n",
    "    steps_to_skip = 0\n",
    "    tokens_used = 0\n",
    "\n",
    "    for i in range(1, len(ids)):\n",
    "        if steps_to_skip > 0:\n",
    "            steps_to_skip -= 1\n",
    "            continue\n",
    "\n",
    "        if np.random.random() < chance:\n",
    "            # Get the token we are using for this space\n",
    "            mask_token = tokenizer.convert_tokens_to_ids(new_tokens[tokens_used])\n",
    "            tokens_used += 1\n",
    "\n",
    "            span = np.random.choice(span_length)\n",
    "            old_toks.append(mask_token)\n",
    "            old_toks.extend(ids[i:i+span].copy())\n",
    "            ids = ids[:i] + [mask_token] + ids[i+span:]\n",
    "\n",
    "            steps_to_skip = span\n",
    "        \n",
    "    \n",
    "    return ids, old_toks\n",
    "\n",
    "def s_denoising(ids, tokenizer):\n",
    "    # Get the length of our input\n",
    "    len_ids = len(ids)\n",
    "\n",
    "    # Build Gaussian distribution of probabilities for each token\n",
    "    p = norm.pdf(np.arange(len_ids)/len_ids, loc=np.mean(np.arange(len_ids)/len_ids), scale=np.std(np.arange(len_ids)/len_ids))\n",
    "\n",
    "    # Normalize the probabilities and get the index: to remove.\n",
    "    remove_index = np.random.choice(np.arange(len_ids), p=p/p.sum())\n",
    "\n",
    "    # Get the token we are using for this space\n",
    "    mask_token = tokenizer.convert_tokens_to_ids(new_tokens[0])\n",
    "\n",
    "    # Get the tokens we are removing\n",
    "    old_tok = [mask_token]\n",
    "    old_tok.extend(ids[remove_index:].copy())\n",
    "\n",
    "    # Mask the tokens\n",
    "    ids = ids[:remove_index]\n",
    "    ids[-1] = mask_token\n",
    "    return ids, old_tok\n",
    "\n",
    "def x_denoising(ids, tokenizer, corruption_pct=0.50, span_length=np.arange(2,6)):\n",
    "    return r_denoising(ids, tokenizer, corruption_pct, span_length)\n",
    "\n",
    "\n",
    "\n",
    "item = next(iter(dataset))['text']\n",
    "\n",
    "'''Testing R-Denoising'''\n",
    "# ids, old_toks = r_denoising(ids, tokenizer, corruption_pct=0.15, span_length=np.arange(2,6))\n",
    "# old_toks\n",
    "\n",
    "'''Testing S-Denoising'''\n",
    "# ids, old_tok = s_denoising(ids, tokenizer)\n",
    "# tokenizer.decode(ids)\n",
    "\n",
    "'''Testing X-Denoising'''\n",
    "# ids, old_toks = x_denoising(ids, tokenizer, corruption_pct=0.50, span_length=np.arange(2,6))\n",
    "# tokenizer.decode(ids)\n",
    "\n",
    "'''Testing all at once'''\n",
    "token_dict = {'s': ['[S2S]', s_denoising], 'r': ['[NLU]', r_denoising], 'x': ['[NLG]', x_denoising]}\n",
    "\n",
    "# Get the token to prepend and function to use.\n",
    "begin_id, func = token_dict[np.random.choice(['s', 'r', 'x'], size=1, p=[0.5, 0.25, 0.25])[0]]\n",
    "\n",
    "# Prepend token to string and tokenize\n",
    "item = begin_id + ' ' + item\n",
    "ids = tokenizer(item, truncation=True, max_length=1024)['input_ids']\n",
    "\n",
    "\n",
    "ids, old_toks = func(ids, tokenizer)\n",
    "tokenizer.decode(ids)\n",
    "print('Total non-tokenized', len(ids))\n",
    "print(f'Total tokenized {len(old_toks)}')\n",
    "print(f'Total tokens {len(ids) + len(old_toks)}')\n",
    "print(tokenizer.decode(ids[0]))\n",
    "tokenizer.decode(old_toks)\n",
    "\n",
    "\n",
    "# old_toks_dec = [tokenizer.decode(tok) for tok in old_toks]\n",
    "# old_toks_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[NLU] \\\\section{Introduction}\\n\\\\label new_id_0 intro}\\n\\n\\\\emph{Gender diversity}, or more often its lack thereof, among participants to\\nsoftware development activities has been thoroughly studied in recent years. new_id_1 particular, the presence of, effects of, and countermeasures for \\\\emph{gender\\n  bias} in Free/Open new_id_2 OSS) have received a lot of attention\\nover the past decade~\\\\cite{david2008f new_id_3, qiu2010kdewomen new_id_4  nafus2012patches, kuechler2012genderfoss, new_id_5 cu2014gender, new_id_6  oneil2016debiansurvey, robles2016womeninfoss, terrell2017gender,\\n  zac new_id_7 2021gender}.  \\\\em new_id_8 } is on the other hand the\\nkind new_id_9  that stems from participants in some global activity coming\\nfrom different world regions and cultures.\\n\\n new_id_10  FOSS has received relatively little attention in scholarly\\nworks. In particular, while seminal survey-based and\\npoint-in-time medium-scale new_id_11  the geographic origins of FOSS\\ncontributors exist new_id_12 cite{ghosh2005understanding, david2008fossdevs,\\n  barah new_id_13 odiversity, takhteyev2010ossge new_id_14  robles2014surveydataset,\\n  wachs2021ossgeography}, large new_id_15  studies of the geographic\\norigin of F new_id_16  still lacking. Such a quantitative\\ncharacterization would be useful to inform decisions related to global\\ndevelopment teams~\\\\cite{herbsleb2007globalsweng} and hiring strategies in the\\ninformation technology (IT) market, as well as contribute factual information\\nto the debates on the economic impact and sociology of FOSS around new_id_17 \\n\\n\\n\\\\paragraph{Contributions}\\n\\nWith this work we contribute to close this new_id_18  conducting \\\\textbf{the first\\n  longitudinal study of the geographic origin of contributors to public code\\n  over 50 years.} Specifically, we provide a preliminary answer to the\\nfollowing research question:\\n\\\\begin{research new_id_19  From which world regions do authors of publicly available commits come from\\n  and how has it changed over the past 50 years?\\n  \\\\label{rq:geod new_id_20 {researchquestion}\\nWe use as dataset the \\\\SWH/ archive~\\\\cite{swhipres2017} and analyze new_id_21 2.2 new_id_22  archived from 160 million\\\\xspace projects and authored by\\n43 million\\\\xspace authors during the 1971--2021 time period. \\nWe geolocate developers to\\n\\\\DATAWorldReg new_id_23  world regions, using as signals email country code top-level domains (ccTLDs) and \\nauthor (first/ new_id_24  names compared with name distributions around the world, and UTC offsets \\nmined from commit metadata.\\n\\nWe find evidence of the early dominance of North America in open source\\nsoftware, later joined by Europe. After that period, new_id_25  \\nin public code has been constantly increasing.\\nWe also identify relevant historical shifts\\nrelated to the end of the UNIX wars and the increase of coding literacy in\\nCentral and South Asia, as well as of broader phenomena like colonialism and\\npeople movement across countries (immigration/emigration).\\n\\n\\n\\n\\n\\\\paragraph{Data availability.}\\n\\nA replication package for this paper is available from Zenodo at\\n\\\\url{https://doi.org/10.5281/zenodo.6390355}~\\\\cite{replication-package}.\\n\\n\\n \\\\section{Related Work}\\n\\\\label{ new_id_26 related}\\n\\n new_id_27 ~\\\\cite{ghosh2005understanding new_id_28 fossdevs,\\n  robles2014surveydataset new_id_29 debiansurvey} have characterized the\\ngeography of Free/Open Source Software (FOSS) using \\\\ new_id_30 {developer surveys},\\nwhich provide high-quality answers but are limited in size (2-5\\\\,K new_id_31 \\nand can be biased by participant sampling.\\n\\nIn 2008 Barahona et al.~\\\\cite{barahona2008geodiversity} conducted a seminal\\nlarge-scale (for the time new_id_32  on FOSS \\\\emph{geography using mining software\\n  repositories (MSR) techniques}. They analyzed the origin of 1\\\\,M contributors\\nusing the SourceForge user database and mailing list archives over the\\n1999--2005 period, using as signals information similar to ours: email domains\\nand UTC offsets. \\nThe studied period (7 years) in~\\\\cite{barahona2008geodiversity} is new_id_33 new_id_34 new_id_35'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50260,\n",
       " 50261,\n",
       " 50262,\n",
       " 50263,\n",
       " 50264,\n",
       " 50265,\n",
       " 50266,\n",
       " 50267,\n",
       " 50268,\n",
       " 50269,\n",
       " 50270,\n",
       " 50271,\n",
       " 50272,\n",
       " 50273,\n",
       " 50274,\n",
       " 50275,\n",
       " 50276,\n",
       " 50277,\n",
       " 50278,\n",
       " 50279,\n",
       " 50280,\n",
       " 50281,\n",
       " 50282,\n",
       " 50283,\n",
       " 50284,\n",
       " 50285,\n",
       " 50286,\n",
       " 50287,\n",
       " 50288,\n",
       " 50289,\n",
       " 50290,\n",
       " 50291,\n",
       " 50292,\n",
       " 50293,\n",
       " 50294,\n",
       " 50295,\n",
       " 50296,\n",
       " 50297,\n",
       " 50298,\n",
       " 50299,\n",
       " 50300,\n",
       " 50301,\n",
       " 50302,\n",
       " 50303,\n",
       " 50304,\n",
       " 50305,\n",
       " 50306,\n",
       " 50307,\n",
       " 50308,\n",
       " 50309,\n",
       " 50310,\n",
       " 50311,\n",
       " 50312,\n",
       " 50313,\n",
       " 50314,\n",
       " 50315,\n",
       " 50316,\n",
       " 50317,\n",
       " 50318,\n",
       " 50319,\n",
       " 50320,\n",
       " 50321,\n",
       " 50322,\n",
       " 50323,\n",
       " 50324,\n",
       " 50325,\n",
       " 50326,\n",
       " 50327,\n",
       " 50328,\n",
       " 50329,\n",
       " 50330,\n",
       " 50331,\n",
       " 50332,\n",
       " 50333,\n",
       " 50334,\n",
       " 50335,\n",
       " 50336,\n",
       " 50337,\n",
       " 50338,\n",
       " 50339,\n",
       " 50340,\n",
       " 50341,\n",
       " 50342,\n",
       " 50343,\n",
       " 50344,\n",
       " 50345,\n",
       " 50346,\n",
       " 50347,\n",
       " 50348,\n",
       " 50349,\n",
       " 50350,\n",
       " 50351,\n",
       " 50352,\n",
       " 50353,\n",
       " 50354,\n",
       " 50355,\n",
       " 50356,\n",
       " 50357,\n",
       " 50358,\n",
       " 50359,\n",
       " 50360,\n",
       " 50361,\n",
       " 50362,\n",
       " 50363,\n",
       " 50364,\n",
       " 50365,\n",
       " 50366,\n",
       " 50367,\n",
       " 50368,\n",
       " 50369,\n",
       " 50370,\n",
       " 50371,\n",
       " 50372,\n",
       " 50373,\n",
       " 50374,\n",
       " 50375,\n",
       " 50376,\n",
       " 50377,\n",
       " 50378,\n",
       " 50379,\n",
       " 50380,\n",
       " 50381,\n",
       " 50382,\n",
       " 50383,\n",
       " 50384,\n",
       " 50385,\n",
       " 50386,\n",
       " 50387,\n",
       " 50388,\n",
       " 50389,\n",
       " 50390,\n",
       " 50391,\n",
       " 50392,\n",
       " 50393,\n",
       " 50394,\n",
       " 50395,\n",
       " 50396,\n",
       " 50397,\n",
       " 50398,\n",
       " 50399,\n",
       " 50400,\n",
       " 50401,\n",
       " 50402,\n",
       " 50403,\n",
       " 50404,\n",
       " 50405,\n",
       " 50406,\n",
       " 50407,\n",
       " 50408,\n",
       " 50409,\n",
       " 50410,\n",
       " 50411,\n",
       " 50412,\n",
       " 50413,\n",
       " 50414,\n",
       " 50415,\n",
       " 50416,\n",
       " 50417,\n",
       " 50418,\n",
       " 50419,\n",
       " 50420,\n",
       " 50421,\n",
       " 50422,\n",
       " 50423,\n",
       " 50424,\n",
       " 50425,\n",
       " 50426,\n",
       " 50427,\n",
       " 50428,\n",
       " 50429,\n",
       " 50430,\n",
       " 50431,\n",
       " 50432,\n",
       " 50433,\n",
       " 50434,\n",
       " 50435,\n",
       " 50436,\n",
       " 50437,\n",
       " 50438,\n",
       " 50439,\n",
       " 50440,\n",
       " 50441,\n",
       " 50442,\n",
       " 50443,\n",
       " 50444,\n",
       " 50445,\n",
       " 50446,\n",
       " 50447,\n",
       " 50448,\n",
       " 50449,\n",
       " 50450,\n",
       " 50451,\n",
       " 50452,\n",
       " 50453,\n",
       " 50454,\n",
       " 50455,\n",
       " 50456,\n",
       " 50457,\n",
       " 50458,\n",
       " 50459]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
